# Zathras Post-Processing Implementation TODO

---

## ⚠️ IMPORTANT: DO NOT EDIT THIS FILE

**This is a tracking document for implementation progress.**

**LLM Instructions:**
- ✅ **DO**: Check off items as they are completed (change `[ ]` to `[x]`)
- ✅ **DO**: Add notes about completion under tasks if helpful
- ❌ **DO NOT**: Modify task descriptions, priorities, or order
- ❌ **DO NOT**: Add new tasks without explicit user instruction
- ❌ **DO NOT**: Remove tasks without explicit user instruction

**User**: You can edit this file freely as needed to adjust priorities, scope, or add/remove tasks.

---

## Overview

This TODO list tracks the implementation of the Zathras post-processing pipeline that converts benchmark results into JSON documents using an **object-based schema** for export to OpenSearch and Horreum.

**Key Architectural Decisions:**
- **Fully denormalized** documents (one per test execution)
- **Object-based structure** with dynamic keys (`runs.run_1`, `runs.run_2`)
- **Timestamps as keys** for time series data
- **No nested arrays** (avoids OpenSearch performance issues)
- **Single unified index** for all benchmark types

---

## Phase 1: Foundation & Utilities

### [x] Task 1: Base Schema Definition Module
**File:** `post_processing/schema.py`  
**Purpose:** Define the object-based schema structure as Python dataclasses/types  
**Priority:** HIGH  
**Dependencies:** None

**What to build:**
```python
# Document structure with metadata, test, SUT, results
# Run object structure with dynamic keys
# Time series object structure with timestamp keys
# Validation helpers
# JSON serialization utilities
```

**Deliverables:**
- Schema classes/dataclasses for all document sections
- Validation functions
- JSON serialization helpers
- Type hints throughout

**Status:** ✅ COMPLETED - 430 lines, 18 dataclasses, validation and JSON serialization implemented

---

### [x] Task 2: Archive Handler Utility
**File:** `post_processing/utils/archive_handler.py`  
**Purpose:** Extract and manage ZIP/TAR archives from Zathras results  
**Priority:** HIGH  
**Dependencies:** None

**What to build:**
```python
def extract_result_archive(zip_path: str) -> dict:
    """
    Extract results_{test}.zip → results_{test}_.tar → test_results/
    Returns: {
        "test_name": "coremark",
        "extracted_path": "/path/to/coremark_2025.11.06/",
        "files": {...}
    }
    """

def extract_sysconfig_archive(tar_path: str) -> dict:
    """Extract sysconfig_info.tar"""

def cleanup_temp_files(path: str):
    """Clean up extracted files"""
```

**Key Functions:**
- Extract nested archives (ZIP → TAR → files)
- Return structured file listings
- Temp directory management
- Cleanup utilities

**Status:** ✅ COMPLETED - 370 lines, ArchiveHandler class with context manager support

---

### [x] Task 3: Parser Utilities
**File:** `post_processing/utils/parser_utils.py`  
**Purpose:** Common parsing functions for various file formats  
**Priority:** HIGH  
**Dependencies:** None

**What to build:**
```python
def parse_csv_timeseries(csv_path: str) -> list:
    """Parse results_*.csv files"""

def parse_key_value_text(text: str) -> dict:
    """Parse run*_summary files"""

def parse_proc_file(file_path: str) -> dict:
    """Parse /proc/* style files"""

def parse_test_times(file_path: str) -> dict:
    """Parse test_times file"""

def parse_command_file(file_path: str) -> dict:
    """Parse {test}.cmd files"""
```

**Parsing Formats:**
- CSV (with various delimiters)
- Key-value text files
- Proc-style files
- Command files
- Structured text

**Status:** ✅ COMPLETED - 420 lines, 15+ parsing functions with automatic type conversion

---

### [x] Task 4: Metadata Extractor
**File:** `post_processing/utils/metadata_extractor.py`  
**Purpose:** Extract and structure SUT metadata from sysconfig files  
**Priority:** HIGH  
**Dependencies:** `parser_utils.py`

**What to build:**
```python
class MetadataExtractor:
    def extract_hardware_metadata(sysconfig_dir: str) -> dict:
        """
        Parse:
        - lscpu.json (already JSON)
        - lshw.json (already JSON)
        - lsmem.json (already JSON)
        - proc_cpuinfo.out
        - proc_meminfo.out
        - dmidecode.out
        - numactl.out
        
        Returns object-based structure:
        {
            "cpu": {...},
            "memory": {...},
            "numa": {"node_0": {}, "node_1": {}},
            "storage": {"device_0": {}, "device_1": {}}
        }
        """
    
    def extract_os_metadata(sysconfig_dir: str) -> dict:
        """Parse OS-related files"""
    
    def extract_config_metadata(sysconfig_dir: str) -> dict:
        """Parse configuration files"""
```

**Handles:**
- JSON files (direct load)
- Proc files (custom parsing)
- NUMA topology
- Storage devices (numbered objects)
- Network interfaces (numbered objects)

**Status:** ✅ COMPLETED - 520 lines, MetadataExtractor class with object-based output

---

## Phase 2: Core Processing

### [ ] Task 5: Base Processor Class
**File:** `post_processing/processors/base_processor.py`  
**Purpose:** Abstract base class for all test processors  
**Priority:** HIGH  
**Dependencies:** `schema.py`, `metadata_extractor.py`, `parser_utils.py`

**What to build:**
```python
class BaseProcessor(ABC):
    def __init__(self, result_directory: str):
        """Initialize with Zathras result directory"""
    
    @abstractmethod
    def parse_runs(self) -> dict:
        """Parse test-specific run data - must be implemented by subclass"""
    
    def extract_metadata(self) -> dict:
        """Extract metadata (common across all tests)"""
    
    def extract_test_config(self) -> dict:
        """Extract test configuration from ansible_vars.yml"""
    
    def build_document(self) -> dict:
        """
        Build complete document with object-based schema:
        {
            "metadata": {...},
            "test": {...},
            "system_under_test": {...},
            "test_configuration": {...},
            "results": {
                "runs": {
                    "run_1": {...},
                    "run_2": {...}
                }
            }
        }
        """
    
    def validate(self, document: dict) -> bool:
        """Validate document structure"""
```

**Key Responsibilities:**
- Define common extraction logic
- Abstract test-specific parsing
- Build complete document structure
- Validation and error handling

---

### [ ] Task 6: CoreMark Processor
**File:** `post_processing/processors/coremark_processor.py`  
**Purpose:** Process CoreMark benchmark results into object-based schema  
**Priority:** HIGH  
**Dependencies:** `base_processor.py`

**What to build:**
```python
class CoreMarkProcessor(BaseProcessor):
    def parse_runs(self) -> dict:
        """
        Parse:
        - results_coremark.csv → time series data
        - run1_summary, run2_summary → metrics
        - run*_iter*.log → optional detailed data
        - version, tuned_setting → metadata
        
        Returns:
        {
            "run_1": {
                "run_number": 1,
                "metrics": {...},
                "timeseries": {
                    "2025-11-06T05:09:45.000Z": {
                        "sequence": 0,
                        "iteration": 1,
                        "value": 193245.201809
                    }
                }
            },
            "run_2": {...}
        }
        """
    
    def calculate_overall_statistics(runs: dict) -> dict:
        """Calculate statistics across all runs"""
```

**Specific Parsing:**
- CSV to timestamp-keyed objects
- Summary files to metrics
- Validation checksums
- Statistics calculation

---

### [ ] Task 7: Test CoreMark Processor
**Purpose:** Validate CoreMark processor with real sample data  
**Priority:** HIGH  
**Dependencies:** `coremark_processor.py`

**Test Plan:**
1. Load `quick_sample_data/rhel/local/localhost_0/results_coremark.zip`
2. Extract and parse all components
3. Verify object-based structure
4. Verify timestamp keys in timeseries: `"2025-11-06T05:09:45.000Z"`
5. Verify sequence numbers: 0, 1, 2, 3, 4...
6. Verify runs object: `{"run_1": {...}, "run_2": {...}}`
7. Validate against schema
8. Export to JSON file
9. Manually inspect output for correctness

**Success Criteria:**
- Valid JSON output
- Object structure matches spec
- Time series properly formatted
- All metadata extracted

---

## Phase 3: OpenSearch & Horreum Integration

### [ ] Task 8: OpenSearch Index Template
**File:** `post_processing/config/opensearch_index_template.json`  
**Purpose:** Define OpenSearch mapping for object-based schema  
**Priority:** MEDIUM  
**Dependencies:** `schema.py`

**What to build:**
```json
{
  "index_patterns": ["zathras-results*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.mapping.total_fields.limit": 3000,
      "index.mapping.nested_objects.limit": 100
    },
    "mappings": {
      "dynamic": "strict",
      "properties": {
        "metadata": {...},
        "test": {...},
        "system_under_test": {...},
        "results": {
          "properties": {
            "runs": {
              "type": "object",
              "dynamic": true
            }
          }
        }
      },
      "dynamic_templates": [
        {
          "run_metrics": {
            "path_match": "results.runs.*.metrics.*",
            "mapping": {"type": "double"}
          }
        },
        {
          "run_timeseries_values": {
            "path_match": "results.runs.*.timeseries.*.value",
            "mapping": {"type": "double"}
          }
        }
      ]
    }
  }
}
```

**Key Mappings:**
- `results.runs` as object type (NOT nested)
- Dynamic templates for run fields
- Appropriate field limits
- Type definitions for common fields

---

### [ ] Task 9: Update OpenSearch Exporter
**File:** `post_processing/exporters/opensearch_exporter.py`  
**Purpose:** Update exporter to work with object-based documents  
**Priority:** MEDIUM  
**Dependencies:** `opensearch_exporter.py` (exists), Task 8

**What to update:**
- Add index template creation method
- Update `index_document()` to handle object structure
- Add validation before indexing
- Add error handling for dynamic field limits
- Handle timestamp key formatting
- Test with sample documents

**New Methods:**
```python
def create_index_template(template_path: str):
    """Create/update index template"""

def validate_document_structure(document: dict) -> bool:
    """Validate object-based structure"""
```

---

### [ ] Task 10: Update Horreum Exporter
**File:** `post_processing/exporters/horreum_exporter.py`  
**Purpose:** Update exporter for object-based documents  
**Priority:** MEDIUM  
**Dependencies:** `horreum_exporter.py` (exists)

**What to update:**
- Define Horreum schema with extractors for object paths
- Update `submit_run()` to handle object structure
- Add label creation for test types
- Handle timestamp-keyed time series
- Test with sample documents

**Schema Extractors:**
```json
{
  "extractors": [
    {"name": "test_name", "jsonpath": "$.test.name"},
    {"name": "run_1_metric", "jsonpath": "$.results.runs.run_1.metrics.iterations_per_second"},
    {"name": "cpu_vendor", "jsonpath": "$.system_under_test.hardware.cpu.vendor"}
  ]
}
```

---

## Phase 4: Orchestration

### [ ] Task 11: Main Orchestrator
**File:** `post_processing/main.py`  
**Purpose:** End-to-end pipeline for processing Zathras results  
**Priority:** HIGH  
**Dependencies:** All processors, exporters

**What to build:**
```python
def process_result_directory(directory: str, config: dict) -> dict:
    """
    1. Discover result files (results_*.zip)
    2. For each result:
       a. Determine test type from filename
       b. Load appropriate processor
       c. Extract and parse
       d. Build document with object-based schema
       e. Validate structure
       f. Export to OpenSearch/Horreum
    3. Return summary of processed results
    """

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Result directory")
    parser.add_argument("--opensearch", action="store_true")
    parser.add_argument("--horreum", action="store_true")
    parser.add_argument("--output-json", help="Write JSON to file")
    parser.add_argument("--config", help="Config file")
    args = parser.parse_args()
    
    # Process and export
```

**Features:**
- Auto-detect test types
- Batch processing
- Progress reporting
- Error handling and recovery
- Summary statistics

---

## Phase 5: Additional Processors

### [ ] Task 12: Pig Processor
**File:** `post_processing/processors/pig_processor.py`  
**Purpose:** Process Pig benchmark results  
**Priority:** LOW  
**Dependencies:** `base_processor.py`

**Note:** Limited sample data available in `results_pig.zip`  
**Status:** Implement after CoreMark validated

---

### [ ] Task 13: FIO Processor
**File:** `post_processing/processors/fio_processor.py`  
**Purpose:** Process FIO benchmark results  
**Priority:** LOW  
**Dependencies:** `base_processor.py`

**Note:** No sample data yet, may need to generate  
**Special Considerations:** FIO produces large time series, may need sampling

---

### [ ] Task 14: STREAMS Processor
**File:** `post_processing/processors/streams_processor.py`  
**Purpose:** Process STREAMS benchmark results  
**Priority:** LOW  
**Dependencies:** `base_processor.py`

**Note:** No sample data yet (test failed in VM run)  
**Metrics:** Copy, Scale, Add, Triad bandwidth

---

## Phase 6: Testing & Validation

### [ ] Task 15: OpenSearch Integration Tests
**File:** `tests/test_opensearch_integration.py`  
**Purpose:** End-to-end testing with OpenSearch  
**Priority:** MEDIUM  
**Dependencies:** Tasks 6-11

**Test Cases:**
1. Index template creation
2. Document indexing
3. Query tests:
   - Range queries on metrics
   - Term queries on fields
   - Aggregations across runs
   - Object path queries: `results.runs.run_1.metrics.*`
4. Time series data retrieval
5. Performance benchmarks
6. Large document handling

**Success Criteria:**
- All documents index successfully
- Queries return expected results
- Performance acceptable (< 1s for typical queries)

---

### [ ] Task 16: Horreum Integration Tests
**File:** `tests/test_horreum_integration.py`  
**Purpose:** End-to-end testing with Horreum  
**Priority:** MEDIUM  
**Dependencies:** Tasks 6-11

**Test Cases:**
1. Schema creation
2. Run submission
3. Extractor validation
4. Query and comparison tests
5. Label assignment
6. Change detection

**Success Criteria:**
- Runs submit successfully
- Extractors pull correct values
- Comparisons work across runs

---

## Phase 7: Documentation & Polish

### [ ] Task 17: User Documentation
**File:** `docs/QUERYING_GUIDE.md`  
**Purpose:** Guide users on querying object-based schema  
**Priority:** MEDIUM  
**Dependencies:** Tasks 8-11

**Content Sections:**
1. Schema overview with examples
2. Common query patterns
3. Accessing time series data
4. Aggregation examples
5. Kibana dashboard examples
6. Troubleshooting guide

---

### [ ] Task 18: Update DATA_ANALYSIS.md
**File:** `DATA_ANALYSIS.md`  
**Purpose:** Update with final object-based schema decisions  
**Priority:** LOW  
**Dependencies:** Task 7

**Updates:**
- Replace nested array examples with object-based
- Add timestamp-keyed time series examples
- Document final schema decisions

---

### [ ] Task 19: Requirements File
**File:** `post_processing/requirements.txt`  
**Purpose:** List all Python dependencies  
**Priority:** MEDIUM  
**Dependencies:** None

**Dependencies to include:**
```txt
pyyaml>=6.0
opensearch-py>=2.0.0
requests>=2.28.0
python-dateutil>=2.8.0
```

---

### [ ] Task 20: CLI Interface
**File:** `post_processing/cli.py`  
**Purpose:** User-friendly command-line interface  
**Priority:** LOW  
**Dependencies:** `main.py`

**Commands:**
```bash
zathras-process --input /path/to/results --opensearch --config config.yml
zathras-process --input /path/to/results --horreum --output results.json
zathras-process --validate /path/to/results
```

---

## Recommended Implementation Order

### **Week 1: Foundation** (HIGH PRIORITY) ✅ COMPLETE
- [x] Task 1 - Schema definition
- [x] Task 2 - Archive handler
- [x] Task 3 - Parser utils
- [x] Task 4 - Metadata extractor

### **Week 2: Core Processing** (HIGH PRIORITY)
- [ ] Task 5 - Base processor
- [ ] Task 6 - CoreMark processor
- [ ] Task 7 - Test with real data

### **Week 3: Integration** (MEDIUM PRIORITY)
- [ ] Task 11 - Main orchestrator
- [ ] Task 8 - OpenSearch template
- [ ] Task 9 - Update OpenSearch exporter
- [ ] Task 19 - Requirements.txt

### **Week 4: Testing & Refinement** (MEDIUM PRIORITY)
- [ ] Task 15 - OpenSearch integration tests
- [ ] Task 10 - Update Horreum exporter
- [ ] Task 16 - Horreum integration tests

### **Week 5+: Extensions** (LOW PRIORITY)
- [ ] Tasks 12-14 - Additional processors
- [ ] Tasks 17-18 - Documentation
- [ ] Task 20 - CLI interface

---

## Success Criteria

- [ ] **Phase 1-2 Complete**: CoreMark processor generates valid object-based JSON (Phase 1 ✅)
- [ ] **Phase 3 Complete**: Documents successfully indexed in OpenSearch with correct mappings
- [ ] **Phase 4 Complete**: End-to-end pipeline processes sample data directory
- [ ] **Phase 5 Complete**: At least 3 benchmark types supported
- [ ] **Phase 6 Complete**: All integration tests pass
- [ ] **Phase 7 Complete**: Documentation complete, ready for team use

---

## Key Files Reference

**Sample Data:**
- `quick_sample_data/rhel/local/localhost_0/results_coremark.zip` - CoreMark results
- `quick_sample_data/rhel/local/localhost_0/results_pig.zip` - Pig results
- `quick_sample_data/rhel/local/localhost_0/sysconfig_info.tar` - System metadata
- `quick_sample_data/rhel/local/localhost_0/ansible_vars.yml` - Test configuration

**Design Documents:**
- `DATA_ANALYSIS.md` - Complete data analysis
- `README.md` - Project overview
- Architecture decision in GitHub issue

**Existing Code:**
- `post_processing/exporters/opensearch_exporter.py` - OpenSearch export (needs update)
- `post_processing/exporters/horreum_exporter.py` - Horreum export (needs update)

---

## Notes

- All processors must handle missing data gracefully
- Timestamp format: ISO 8601 with milliseconds: `"2025-11-06T05:09:45.000Z"`
- Dynamic keys format: `run_1`, `run_2`, `node_0`, `device_0`, etc.
- Include `sequence` fields for explicit ordering
- Object type (not nested) for all dynamic key structures
- Validate documents before export
- Log all errors with context

---

**Last Updated:** 2025-11-06  
**Status:** Phase 1 Complete (4/20 tasks) - Ready for Phase 2

**Phase 1 Summary:**
- ✅ 4 tasks completed
- ✅ ~1,740 lines of code written
- ✅ Object-based schema fully defined
- ✅ All foundation utilities implemented
- ✅ Ready for processor development

